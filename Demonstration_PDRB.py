# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bDpSMu2TwGYjoQHiuKecxSGktZ9yfkmQ

### Import Library
"""

!wget https://raw.githubusercontent.com/saffanaha1/MSNBurr_for_PyMC/main/MSNBurr_distribution.py -O MSNBurr_distribution.py
from MSNBurr_distribution import msnburr as msnburr
import xarray as xr
import pymc as pm
import numpy as np
import arviz as az
import pandas as pd
import seaborn as sns
import statsmodels.api as sm
import pytensor.tensor as pt
import matplotlib.pyplot as plt
from scipy import stats
from scipy.stats import skew
from sklearn.metrics import mean_squared_error

"""### Load Data"""

pdrb = 'https://raw.githubusercontent.com/saffanaha1/MSNBurr_for_PyMC/main/Data%20PDRB.xlsx'
data1 = pd.read_excel(pdrb)
data = data1.iloc[:,1:3]
data.head()

y = np.log(data["Y"])
x1 = np.log(data["X1"])

"""### Cek dengan OLS

"""

# 1. Mengimpor Pustaka
import numpy as np
import pandas as pd
import statsmodels.api as sm
import matplotlib.pyplot as plt
from scipy.stats import skew
from sklearn.metrics import mean_squared_error

# 2. Menyiapkan Data
X = np.log(data[["X1"]])
Y = np.log(data["Y"])

# 3. Menambahkan Kolom Konstan
X = sm.add_constant(X)

# 4. Membuat Model Regresi OLS dan Melakukan Fitting
model = sm.OLS(Y, X).fit()

# 5. Melihat Ringkasan Hasil Regresi
print(model.summary())

# 6. Menghitung Residual
residuals = model.resid

y_pred = model.predict(X)
# 7. Menghitung Mean Squared Error (MSE)
mse = mean_squared_error(Y, y_pred)
print(f'Mean Squared Error (MSE): {mse}')

plt.hist(residuals, edgecolor='black')  # Customize bins, color, and transparency
plt.title('Histogram of Residuals')
plt.show()

"""### Uji Normalitas"""

import scipy.stats as stats
shapiro_test_result = stats.shapiro(data)
print(f"Shapiro-Wilk test (p-value): {shapiro_test_result[1]}")

"""### Uji Linieritas"""

import rpy2.robjects as ro
from rpy2.robjects.packages import importr
from rpy2.robjects.vectors import FloatVector
import rpy2.robjects.numpy2ri

utils = importr('utils')
utils.install_packages('tseries')

# Aktifkan konversi numpy
rpy2.robjects.numpy2ri.activate()

# Impor paket tseries dari R
tseries = importr('tseries')

# Convert numpy arrays to R vectors
x1_r = FloatVector(x1)
# x2_r = FloatVector(x2)
# x3_r = FloatVector(x3)
y_r = FloatVector(y)

# Gabungkan x1, x2, x3 menjadi matriks di R
# x_combined_r = ro.r['cbind'](x1_r, x2_r)

# Menjalankan Terasvirta Test
# terasvirta_test_result = tseries.terasvirta_test(x_combined_r, y_r)
terasvirta_test_result = tseries.terasvirta_test(x1_r,y_r)

# Mendapatkan hasil
f_statistic = terasvirta_test_result.rx2('statistic')[0]
p_value = terasvirta_test_result.rx2('p.value')[0]

# Menampilkan hasil
print(f'F-statistic: {f_statistic}, p-value: {p_value}')

df = pd.DataFrame(data)

# Scatter plot X1 vs Y
plt.scatter(df['X1'], df['Y'])
plt.xlabel('X1')
plt.ylabel('Y')
plt.title('Scatter plot of X1 vs Y')
plt.show()

# Pair plot untuk semua variabel
sns.pairplot(df)
plt.show()

# Correlation matrix
correlation_matrix = df.corr()
print(correlation_matrix)

"""### Regresi Bayesian Normal

#### Build Model
"""

with pm.Model() as regresinormal:
    # Prior
    sigma = pm.HalfCauchy('sigma',5)
    b0 = pm.Normal('b0',0, 5)
    b1 = pm.Normal('b1',0, 1)

    eps = b0 + b1*x1

    pm.Normal('Regresi Normal', mu=eps, sigma=sigma, observed=y)
    step = pm.NUTS(target_accept=0.95, max_treedepth=15)
    trace = pm.sample(3000, step=step, chains=4, cores=4,return_inferencedata=True, random_seed=42, tune=2000)

az.plot_trace(trace)

az.summary(trace, round_to=5, hdi_prob=0.95)

summary = az.summary(trace, round_to=3)
mean_values = summary['mean']
print("Nilai mean dari setiap variabel:")
print(mean_values)

"""#### Posterior Predictive Check"""

with regresinormal:
    # Draw samples from posterior predictive
    post_pred = pm.sample_posterior_predictive(trace)
    trace.extend(post_pred)

Y = np.log(data["Y"])
fig, ax = plt.subplots()
x = xr.DataArray(np.linspace(0, 34, 34), dims=["plot_dim"])
y = trace.posterior_predictive['Regresi Normal']

ax.plot(x, y.stack(sample=("chain","draw")).values.squeeze(), c="k", alpha=0.4)
ax.plot(Y, color="r", label="data")
ax.legend();

y = np.log(data["Y"])
x1 = np.log(data["X1"])
y_pred = mean_values[0] + mean_values[1]*x1
residuals = y - y_pred
plt.hist(residuals, edgecolor='black')  # Customize bins, color, and transparency
plt.title('PDRB')
plt.xlabel('Nilai')
plt.ylabel('Frekuensi')
plt.show()

"""### Regresi Bayesian MSNBurr

#### Build Model
"""

y = np.log(data["Y"])
with pm.Model() as regresimsnburr:
    sigma = pm.HalfCauchy('sigma',5)
    alpha = pm.Gamma('alpha',2,0.5)
    b0 = pm.Normal('b0',0, 5)
    b1 = pm.Normal('b1',0, 1)

    eps = b0 + b1*x1

    msnburr('Regresi MSNBurr', mu=eps, sigma=sigma, alpha=alpha, observed=y)
    step = pm.NUTS(target_accept=0.95, max_treedepth=15)
    trace2 = pm.sample(3000, step=step, chains=4, cores=4,return_inferencedata=True, random_seed=42, tune=2000)

az.plot_trace(trace2)

az.summary(trace2, round_to=5, hdi_prob=0.95)

summary = az.summary(trace, round_to=3)
mean_values = summary['mean']
print("Nilai mean dari setiap variabel:")
print(mean_values)

"""#### Posterior Predictive Check"""

with regresimsnburr:
    # Draw samples from posterior predictive
    post_pred = pm.sample_posterior_predictive(trace2)
    trace2.extend(post_pred)

Y = np.log(data["Y"])
fig, ax = plt.subplots()
x = xr.DataArray(np.linspace(0, 34, 34), dims=["plot_dim"])
y = trace2.posterior_predictive['Regresi MSNBurr']

ax.plot(x, y.stack(sample=("chain","draw")).values.squeeze(), c="k", alpha=0.4)
ax.plot(Y, color="r", label="data")
ax.legend();

y = np.log(data["Y"])
x1 = np.log(data["X1"])
y_pred = mean_values[0] + mean_values[1]*x1
residuals = y - y_pred
plt.hist(residuals, edgecolor='black')  # Customize bins, color, and transparency
plt.title('PDRB ADHK')
plt.xlabel('Nilai')
plt.ylabel('Frekuensi')
plt.show()

"""### Cek Kebaikan Model"""

with regresinormal:
    pm.compute_log_likelihood(trace)

waic = az.waic(trace)
loo = az.loo(trace)

print(waic)
print(loo)

with regresimsnburr:
    pm.compute_log_likelihood(trace2)

waic = az.waic(trace2)
loo = az.loo(trace2)

print(waic)
print(loo)